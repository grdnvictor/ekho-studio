// backend/src/controllers/audio/AudioAgentController.ts
import { Request, Response } from "express";
import { audioAgent } from "@/agents/audio/audio-agent";
import { AudioAgentChatContractType } from "@/contracts/api/AudioAgentContracts";
import { ValidatedRequest } from "@/types";
import { HumanMessage } from "@langchain/core/messages";

export class AudioAgentController {
  static async chat(request: Request, response: Response): Promise<void> {
    console.log("üéØ === DEBUT AudioAgentController.chat ===");

    try {
      const validatedRequest = request as ValidatedRequest<AudioAgentChatContractType>;
      const { message, sessionId, context } = validatedRequest.validated.body;

      const finalSessionId = sessionId || `session_${Date.now()}`;

      console.log("üì¶ Donn√©es re√ßues:", {
        message: message?.slice(0, 100),
        sessionId: finalSessionId,
        context
      });

      const config = {
        configurable: {
          thread_id: finalSessionId,
        },
      };

      // Construire le message avec contexte si fourni
      let fullMessage = message;
      if (context) {
        const contextParts = [];
        if (context.targetAudience) contextParts.push(`Public cible: ${context.targetAudience}`);
        if (context.style) contextParts.push(`Style: ${context.style}`);
        if (context.duration) contextParts.push(`Dur√©e: ${context.duration}s`);
        if (context.emotion) contextParts.push(`√âmotion: ${context.emotion}`);

        if (contextParts.length > 0) {
          fullMessage = `${message}\n\nContexte fourni: ${contextParts.join(', ')}`;
        }
      }

      // Cr√©er le message utilisateur
      const userMessage = new HumanMessage(fullMessage);

      console.log("üöÄ Envoi √† l'agent...");

      const result = await audioAgent.invoke(
        { messages: [userMessage] },
        config
      );

      const responseContent = result.messages[0].content;
      const conversationState = result.conversationState;

      console.log("üìù R√©ponse agent:", responseContent?.slice(0, 200));
      console.log("üìä √âtat conversation:", conversationState);

      // Analyser la r√©ponse pour d√©terminer les actions possibles
      const analysis = AudioAgentController.analyzeAgentResponse(
        responseContent as string,
        conversationState
      );

      response.status(200).json({
        success: true,
        sessionId: finalSessionId,
        response: responseContent,
        needsMoreInfo: analysis.needsMoreInfo,
        missingInfo: analysis.missingInfo,
        suggestions: analysis.suggestions,
        nextSteps: analysis.nextSteps,
        canProceed: analysis.canProceed,
        readyToGenerate: analysis.readyToGenerate,
        conversationLength: result.historyLength,
        collectedInfo: conversationState.collectedInfo,
        phase: conversationState.phase
      });

    } catch (error: unknown) {
      console.error("‚ùå Erreur:", error);

      response.status(500).json({
        success: false,
        error: "Erreur lors de la communication avec l'agent audio",
        details: error instanceof Error ? error.message : undefined,
      });
    }
  }

  /**
   * Analyse intelligente de la r√©ponse de l'agent
   */
  private static analyzeAgentResponse(content: string, state: any): {
    needsMoreInfo: boolean;
    missingInfo: string[];
    suggestions: string[];
    nextSteps: string[];
    canProceed: boolean;
    readyToGenerate: boolean;
  } {
    const lowerContent = content.toLowerCase();

    // D√©terminer les infos manquantes bas√© sur l'√©tat
    const missingInfo = [];
    if (!state.hasContent) missingInfo.push('Contenu √† vocaliser');
    if (!state.hasAudience) missingInfo.push('Public cible');
    if (!state.hasDuration) missingInfo.push('Dur√©e souhait√©e');
    if (!state.hasStyle) missingInfo.push('Style/ton');
    if (!state.hasContext) missingInfo.push('Contexte d\'utilisation');

    // Indicateurs que l'agent pose des questions
    const askingQuestions = [
      /quel.*\?/i,
      /quelle.*\?/i,
      /pouvez-vous/i,
      /pourriez-vous/i,
      /avez-vous/i,
      /me dire/i,
      /pr√©ciser/i,
      /plus.*informations/i,
      /\?.*$/m
    ].some(pattern => pattern.test(content));

    // Indicateurs que l'agent est pr√™t √† g√©n√©rer
    const readyToGenerate = [
      /g√©n√©rer/i,
      /cr√©er.*audio/i,
      /proc√©der/i,
      /lancer/i,
      /parfait/i,
      /excellent/i,
      /toutes.*informations/i,
      /pr√™t/i,
      /maintenant/i
    ].some(pattern => pattern.test(content)) && state.phase === 'generation';

    const needsMoreInfo = askingQuestions && !readyToGenerate;
    const canProceed = state.phase === 'generation' || state.phase === 'complete';

    // G√©n√©rer des suggestions bas√©es sur la phase
    let suggestions: string[] = [];
    let nextSteps: string[] = [];

    switch (state.phase) {
      case 'discovery':
        suggestions = [
          "D√©crivez votre projet audio en quelques mots",
          "Mentionnez le type de contenu (pub, formation, etc.)",
          "L'assistant va vous guider √©tape par √©tape"
        ];
        nextSteps = [
          "D√©crire le projet",
          "Fournir le contexte g√©n√©ral"
        ];
        break;

      case 'clarification':
        suggestions = [
          "R√©pondez √† la question pos√©e par l'assistant",
          "Soyez pr√©cis dans votre r√©ponse",
          "Une seule information √† la fois"
        ];
        nextSteps = [
          "R√©pondre √† la question",
          "Clarifier les d√©tails demand√©s"
        ];
        break;

      case 'generation':
        suggestions = [
          "L'assistant peut maintenant g√©n√©rer votre audio",
          "Confirmez pour proc√©der",
          "Vous pourrez √©couter et t√©l√©charger le r√©sultat"
        ];
        nextSteps = [
          "Confirmer la g√©n√©ration",
          "G√©n√©rer l'audio",
          "√âcouter le r√©sultat"
        ];
        break;

      case 'complete':
        suggestions = [
          "Toutes les informations sont collect√©es",
          "Pr√™t pour la g√©n√©ration audio",
          "Personnalisations possibles"
        ];
        nextSteps = [
          "G√©n√©rer l'audio",
          "T√©l√©charger le fichier",
          "Nouvelles variations"
        ];
        break;
    }

    return {
      needsMoreInfo,
      missingInfo,
      suggestions,
      nextSteps,
      canProceed,
      readyToGenerate
    };
  }

  static async generateProject(request: Request, response: Response): Promise<void> {
    response.status(501).json({ error: "Not implemented yet" });
  }

  /**
   * Endpoint pour vider l'historique d'une session
   */
  static async clearHistory(request: Request, response: Response): Promise<void> {
    try {
      const { sessionId } = request.body;
      if (sessionId) {
        audioAgent.clearHistory(sessionId);
        console.log("üóëÔ∏è Historique supprim√© pour session:", sessionId);
      }
      response.json({ success: true, message: "Historique supprim√©" });
    } catch (error) {
      response.status(500).json({ success: false, error: "Erreur lors de la suppression" });
    }
  }
}