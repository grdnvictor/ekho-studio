// backend/src/agents/audio/audio-agent.ts

import { ChatOpenAI } from "@langchain/openai";
import { MemorySaver } from "@langchain/langgraph";
import { SystemMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { audioGenerationTool } from "./tools/audio-generation";
import { recommendVoice } from "../../services/audio/VoiceGuide";

console.log("üöÄ Initialisation de l'agent audio intelligent...");

const LM_STUDIO_URL = process.env.LM_STUDIO_URL || 'http://localhost:1234/v1';
console.log("üåç URL LM Studio:", LM_STUDIO_URL);

const agentModel = new ChatOpenAI({
  temperature: 0.7,
  model: "local-model",
  apiKey: "lm-studio",
  configuration: {
    baseURL: LM_STUDIO_URL
  }
});

const agentWithTools = agentModel.bindTools([audioGenerationTool]);

// Interface pour les donn√©es collect√©es
interface CollectedData {
  projectType?: string;
  textContent?: string;
  targetAudience?: string;
  voiceGender?: string;
  emotionStyle?: string;
  duration?: number;
  context?: string;
  isReadyToGenerate?: boolean;
  conversationContext?: string[];
  currentStep?: string; // Ajout du suivi de l'√©tape actuelle
  hasAskedForText?: boolean; // Flag pour √©viter de redemander le texte
}

// √ânum√©ration des √©tapes
enum ConversationStep {
  INITIAL = 'initial',
  PROJECT_TYPE = 'project_type',
  TEXT_CONTENT = 'text_content',
  TARGET_AUDIENCE = 'target_audience',
  VOICE_PREFERENCE = 'voice_preference',
  STYLE_EMOTION = 'style_emotion',
  CONFIRMATION = 'confirmation',
  GENERATION = 'generation',
  COMPLETE = 'complete'
}

// Interface pour la r√©ponse de l'agent
interface AgentResponse {
  messages: AIMessage[];
  conversationState: {
    phase: string;
    step: number;
  };
  historyLength: number;
  audioGenerated: boolean;
  audioUrl?: string;
  collectedInfo?: string[];
  sessionData?: CollectedData;
}

// Stockage en m√©moire des sessions
const sessionStore = new Map<string, CollectedData>();
const conversationHistory = new Map<string, { role: string; content: string }[]>();

// Prompts optimis√©s par √©tape
const STEP_PROMPTS = {
  [ConversationStep.INITIAL]: [
    "üéôÔ∏è Salut ! Je suis ton assistant audio d'Ekho Studio. Quel type de contenu audio tu veux cr√©er aujourd'hui ? (publicit√©, podcast, formation, narration...)",
    "üéµ Bienvenue sur Ekho Studio ! Dis-moi, c'est pour quel type de projet audio ?",
  ],
  [ConversationStep.TEXT_CONTENT]: [
    "Super choix ! Maintenant, quel est le texte exact que tu veux transformer en audio ?",
    "G√©nial ! Partage-moi le texte que tu veux vocaliser.",
    "Parfait ! Quel message veux-tu faire passer ? Donne-moi le texte complet."
  ],
  [ConversationStep.TARGET_AUDIENCE]: [
    "C'est not√© ! Pour qui est destin√© cet audio ? (grand public, professionnels, enfants...)",
    "Excellent ! Qui va √©couter ton audio ?",
    "Top ! √Ä quel public s'adresse ton message ?"
  ],
  [ConversationStep.VOICE_PREFERENCE]: [
    "Compris ! Quel type de voix pr√©f√®res-tu ? (masculine, f√©minine, jeune, mature...)",
    "D'accord ! Tu pr√©f√®res une voix masculine, f√©minine, ou peu importe ?",
  ],
  [ConversationStep.STYLE_EMOTION]: [
    "Presque fini ! Quel style veux-tu donner √† ton audio ? (dynamique, calme, professionnel, chaleureux...)",
    "Derni√®re touche : quelle ambiance souhaites-tu ?",
  ],
  [ConversationStep.CONFIRMATION]: [
    "üöÄ Parfait ! J'ai tout ce qu'il me faut. On lance la cr√©ation de ton audio ?",
    "‚ú® Super ! Tout est pr√™t. Je peux g√©n√©rer ton audio maintenant ?",
  ]
};

// Fonction pour d√©terminer la prochaine √©tape
function getNextStep(sessionData: CollectedData): ConversationStep {
  if (!sessionData.projectType) return ConversationStep.PROJECT_TYPE;
  if (!sessionData.textContent && !sessionData.hasAskedForText) return ConversationStep.TEXT_CONTENT;
  if (!sessionData.targetAudience) return ConversationStep.TARGET_AUDIENCE;
  if (!sessionData.voiceGender) return ConversationStep.VOICE_PREFERENCE;
  if (!sessionData.emotionStyle) return ConversationStep.STYLE_EMOTION;
  if (!sessionData.isReadyToGenerate) return ConversationStep.CONFIRMATION;
  return ConversationStep.GENERATION;
}

// Fonction pour d√©tecter le type de projet
function detectProjectType(message: string): string | null {
  const lowerMessage = message.toLowerCase();
  const projectTypes = {
    'publicit√©': ['pub', 'publicit√©', 'annonce', 'spot', 'commercial'],
    'podcast': ['podcast', '√©mission', 'chronique'],
    'formation': ['formation', 'cours', 'tutoriel', 'e-learning', '√©ducatif'],
    'narration': ['narration', 'histoire', 'conte', 'documentaire', 'r√©cit'],
    'pr√©sentation': ['pr√©sentation', 'pitch', 'conf√©rence'],
    'audiobook': ['audiobook', 'livre audio', 'lecture']
  };

  for (const [type, keywords] of Object.entries(projectTypes)) {
    if (keywords.some(keyword => lowerMessage.includes(keyword))) {
      return type;
    }
  }
  return null;
}

// Fonction pour d√©tecter le public cible
function detectTargetAudience(message: string): string | null {
  const lowerMessage = message.toLowerCase();
  const audiences = {
    'grand public': ['grand public', 'tout public', 'g√©n√©ral', 'tous'],
    'professionnels': ['professionnel', 'entreprise', 'b2b', 'business', 'corporate'],
    'jeunes': ['jeune', 'ado', 'adolescent', '√©tudiant', 'g√©n√©ration z'],
    'enfants': ['enfant', 'kids', 'jeunesse', 'petit'],
    'familles': ['famille', 'familial', 'parent'],
    'seniors': ['senior', '√¢g√©', 'retrait√©']
  };

  for (const [audience, keywords] of Object.entries(audiences)) {
    if (keywords.some(keyword => lowerMessage.includes(keyword))) {
      return audience;
    }
  }
  return null;
}

// Fonction pour d√©tecter les pr√©f√©rences de voix
function detectVoicePreference(message: string): string | null {
  const lowerMessage = message.toLowerCase();
  if (lowerMessage.includes('masculin') || lowerMessage.includes('homme')) return 'masculine';
  if (lowerMessage.includes('f√©minin') || lowerMessage.includes('femme')) return 'feminine';
  if (lowerMessage.includes('jeune')) return 'jeune';
  if (lowerMessage.includes('mature') || lowerMessage.includes('√¢g√©')) return 'mature';
  if (lowerMessage.includes('peu importe') || lowerMessage.includes('aucune pr√©f√©rence')) return 'neutral';
  return null;
}

// Fonction pour d√©tecter le style √©motionnel
function detectEmotionStyle(message: string): string | null {
  const lowerMessage = message.toLowerCase();
  const styles = {
    'dynamique': ['dynamique', '√©nergique', 'enjou√©', 'enthousiaste'],
    'calme': ['calme', 'pos√©', 'tranquille', 'apaisant', 'zen'],
    'professionnel': ['professionnel', 's√©rieux', 'formel', 'corporate'],
    'chaleureux': ['chaleureux', 'amical', 'sympathique', 'convivial'],
    'dramatique': ['dramatique', 'intense', '√©motionnel', 'passionn√©'],
    'neutre': ['neutre', 'naturel', 'normal']
  };

  for (const [style, keywords] of Object.entries(styles)) {
    if (keywords.some(keyword => lowerMessage.includes(keyword))) {
      return style;
    }
  }
  return null;
}

// Fonction am√©lior√©e pour analyser le message utilisateur
async function analyzeUserMessage(
  message: string,
  sessionData: CollectedData,
  currentStep: ConversationStep
): Promise<Partial<CollectedData>> {
  const detectedInfo: Partial<CollectedData> = {};

  // Analyse bas√©e sur l'√©tape actuelle
  switch (currentStep) {
    case ConversationStep.PROJECT_TYPE:
      const projectType = detectProjectType(message);
      if (projectType) detectedInfo.projectType = projectType;
      break;

    case ConversationStep.TEXT_CONTENT:
      // Le texte est tout ce qui est fourni √† cette √©tape
      if (message.length > 10) {
        detectedInfo.textContent = message.trim();
        detectedInfo.hasAskedForText = true;
      }
      break;

    case ConversationStep.TARGET_AUDIENCE:
      const audience = detectTargetAudience(message);
      if (audience) detectedInfo.targetAudience = audience;
      break;

    case ConversationStep.VOICE_PREFERENCE:
      const voice = detectVoicePreference(message);
      if (voice) detectedInfo.voiceGender = voice;
      break;

    case ConversationStep.STYLE_EMOTION:
      const style = detectEmotionStyle(message);
      if (style) detectedInfo.emotionStyle = style;
      break;

    case ConversationStep.CONFIRMATION:
      const lowerMessage = message.toLowerCase();
      if (lowerMessage.match(/oui|ok|go|lance|g√©n√®re|parfait|c'est bon/)) {
        detectedInfo.isReadyToGenerate = true;
      }
      break;
  }

  // D√©tection opportuniste : si l'utilisateur donne plusieurs infos d'un coup
  if (!sessionData.projectType) {
    const projectType = detectProjectType(message);
    if (projectType) detectedInfo.projectType = projectType;
  }

  // NE PAS d√©tecter automatiquement le texte sauf si on est √† l'√©tape TEXT_CONTENT
  // Cela √©vite de prendre n'importe quelle phrase comme √©tant le texte √† vocaliser

  return detectedInfo;
}

// Fonction pour obtenir une r√©ponse appropri√©e selon l'√©tape
function getStepResponse(step: ConversationStep, sessionData: CollectedData): string {
  const prompts = STEP_PROMPTS[step];
  if (!prompts || prompts.length === 0) {
    return "Dis-moi en plus sur ton projet !";
  }

  // Personnaliser la r√©ponse selon le contexte
  let response = prompts[Math.floor(Math.random() * prompts.length)];

  // Ajouter des encouragements contextuels
  if (sessionData.projectType && step === ConversationStep.TEXT_CONTENT) {
    response = `Une ${sessionData.projectType}, excellent choix ! ${response}`;
  }

  return response;
}

// Fonction pour v√©rifier si on peut passer √† l'√©tape suivante
function canProceedToNextStep(sessionData: CollectedData, currentStep: ConversationStep): boolean {
  switch (currentStep) {
    case ConversationStep.PROJECT_TYPE:
      return !!sessionData.projectType;
    case ConversationStep.TEXT_CONTENT:
      return !!sessionData.textContent;
    case ConversationStep.TARGET_AUDIENCE:
      return !!sessionData.targetAudience;
    case ConversationStep.VOICE_PREFERENCE:
      return !!sessionData.voiceGender;
    case ConversationStep.STYLE_EMOTION:
      return !!sessionData.emotionStyle;
    case ConversationStep.CONFIRMATION:
      return !!sessionData.isReadyToGenerate;
    default:
      return false;
  }
}

export const audioAgent = {
  async invoke(input: any, config: any): Promise<AgentResponse> {
    console.log("ü§ñ Agent intelligent appel√©");

    const threadId: string = config.configurable?.thread_id;
    if (!threadId) {
      throw new Error("Thread ID manquant");
    }

    // R√©cup√©rer ou initialiser les donn√©es de session
    let sessionData: CollectedData = sessionStore.get(threadId) || {
      currentStep: ConversationStep.INITIAL
    };
    let history = conversationHistory.get(threadId) || [];

    // Extraire le message utilisateur
    const userMessage = input.messages?.[input.messages.length - 1];
    const userText: string = userMessage?.content || '';

    console.log("üìä √âtat session:", {
      currentStep: sessionData.currentStep,
      hasText: !!sessionData.textContent,
      hasProjectType: !!sessionData.projectType,
      historyLength: history.length,
      userMessage: userText.slice(0, 50)
    });

    try {
      // Si c'est le premier message
      if (history.length === 0) {
        const welcomeMessage = new AIMessage(getStepResponse(ConversationStep.INITIAL, sessionData));
        history.push({ role: 'assistant', content: welcomeMessage.content as string });
        conversationHistory.set(threadId, history);

        sessionData.currentStep = ConversationStep.PROJECT_TYPE;
        sessionStore.set(threadId, sessionData);

        return {
          messages: [welcomeMessage],
          conversationState: { phase: 'discovery', step: 1 },
          historyLength: 1,
          audioGenerated: false,
          sessionData
        };
      }

      // Ajouter le message utilisateur √† l'historique
      history.push({ role: 'user', content: userText });

      // Analyser le message selon l'√©tape actuelle
      const currentStep = sessionData.currentStep as ConversationStep;
      const detectedInfo = await analyzeUserMessage(userText, sessionData, currentStep);

      console.log("üîç Analyse du message:", {
        currentStep,
        detectedInfo,
        userText: userText.slice(0, 50)
      });

      // Mettre √† jour les donn√©es de session
      sessionData = { ...sessionData, ...detectedInfo };

      // V√©rifier si on peut passer √† l'√©tape suivante
      if (canProceedToNextStep(sessionData, currentStep)) {
        const nextStep = getNextStep(sessionData);
        sessionData.currentStep = nextStep;
        console.log(`‚úÖ Passage √† l'√©tape: ${currentStep} -> ${nextStep}`);
      } else {
        console.log(`‚è∏Ô∏è Reste √† l'√©tape: ${currentStep} (info manquante)`);
      }

      sessionStore.set(threadId, sessionData);

      // Si on est pr√™t √† g√©n√©rer
      if (sessionData.currentStep === ConversationStep.GENERATION && sessionData.isReadyToGenerate) {
        console.log("üéµ G√©n√©ration audio demand√©e");

        const statusMessage = new AIMessage(
          "üéµ Super ! Je lance la g√©n√©ration de ton audio... √áa va prendre quelques secondes ‚è≥"
        );
        history.push({ role: 'assistant', content: statusMessage.content as string });
        conversationHistory.set(threadId, history);

        return await this.generateAudio(sessionData, threadId);
      }

      // G√©n√©rer la r√©ponse appropri√©e pour l'√©tape actuelle
      const response = getStepResponse(sessionData.currentStep as ConversationStep, sessionData);

      // D√©terminer la phase pour le frontend
      let phase = 'discovery';
      if (sessionData.textContent) {
        phase = 'clarification';
      }
      if (sessionData.currentStep === ConversationStep.CONFIRMATION) {
        phase = 'generation';
      }

      // Cr√©er le message de r√©ponse
      const responseMessage = new AIMessage(response);
      history.push({ role: 'assistant', content: response });
      conversationHistory.set(threadId, history);

      // Collecter les infos pour l'affichage
      const collectedInfo: string[] = [];
      if (sessionData.projectType) collectedInfo.push(`Type: ${sessionData.projectType}`);
      if (sessionData.textContent) collectedInfo.push('Texte fourni ‚úì');
      if (sessionData.targetAudience) collectedInfo.push(`Public: ${sessionData.targetAudience}`);
      if (sessionData.voiceGender) collectedInfo.push(`Voix: ${sessionData.voiceGender}`);
      if (sessionData.emotionStyle) collectedInfo.push(`Style: ${sessionData.emotionStyle}`);

      return {
        messages: [responseMessage],
        conversationState: { phase, step: history.length },
        historyLength: history.length,
        audioGenerated: false,
        collectedInfo,
        sessionData
      };

    } catch (error: unknown) {
      console.error("‚ùå Erreur dans l'agent:", error);
      const errorMessage = new AIMessage(
        "üòÖ Oups ! J'ai eu un petit souci. Peux-tu reformuler ta demande ?"
      );

      return {
        messages: [errorMessage],
        conversationState: { phase: 'error', step: history.length },
        historyLength: history.length,
        audioGenerated: false
      };
    }
  },

  async generateAudio(sessionData: CollectedData, threadId: string): Promise<AgentResponse> {
    console.log("üéµ G√©n√©ration audio avec donn√©es:", sessionData);

    try {
      if (!sessionData.textContent) {
        throw new Error("Contenu textuel manquant");
      }

      // Choisir la voix optimale
      const voiceName: string = recommendVoice({
        projectType: sessionData.projectType,
        targetAudience: sessionData.targetAudience,
        emotion: sessionData.emotionStyle,
        gender: sessionData.voiceGender
      });

      // Pr√©parer les param√®tres
      const generationParams = {
        text: sessionData.textContent,
        voiceName: voiceName,
        emotion: sessionData.emotionStyle || 'neutral',
        speed: 1.0,
        effects: []
      };

      console.log("üéØ Param√®tres g√©n√©ration:", generationParams);

      // G√©n√©rer l'audio
      const audioResult = await audioGenerationTool.invoke(generationParams);

      if (audioResult.success) {
        const funMessages = [
          "üéâ Tadaaa ! Ton audio est pr√™t ! J'esp√®re qu'il te plaira autant qu'√† moi !",
          "‚ú® Et voil√† ! J'ai mis tout mon c≈ìur dans cet audio. √âcoute-le vite !",
          "üöÄ Mission accomplie ! Ton audio est fin pr√™t. C'est du lourd !",
          "üéµ Boom ! Audio g√©n√©r√© avec succ√®s ! J'ai h√¢te que tu l'√©coutes !"
        ];

        const summaryMessage = new AIMessage(
          `${funMessages[Math.floor(Math.random() * funMessages.length)]}\n\n` +
          `üìã Petit r√©cap de ton projet :\n` +
          (sessionData.projectType ? `‚Ä¢ Type : ${sessionData.projectType}\n` : '') +
          (sessionData.targetAudience ? `‚Ä¢ Public : ${sessionData.targetAudience}\n` : '') +
          `‚Ä¢ Voix : ${this.getVoiceDisplayName(voiceName)}\n` +
          (sessionData.emotionStyle ? `‚Ä¢ Style : ${this.getStyleDisplayName(sessionData.emotionStyle)}\n` : '') +
          `‚Ä¢ Dur√©e : ~${('duration' in audioResult ? audioResult.duration : 0)}s\n\n` +
          `üéß Tu peux maintenant √©couter et t√©l√©charger ton audio ci-dessus.\n\n` +
          `üí° Envie d'autre chose ? Dis-moi "nouveau" pour cr√©er un autre audio !`
        );

        // Reset pour un nouveau projet
        sessionStore.delete(threadId);
        conversationHistory.delete(threadId);

        return {
          messages: [summaryMessage],
          conversationState: { phase: 'complete', step: 6 },
          historyLength: 6,
          audioGenerated: true,
          audioUrl: ('url' in audioResult) ? audioResult.url : undefined,
          sessionData: sessionData
        };
      } else {
        throw new Error(('error' in audioResult ? audioResult.error : audioResult.message) || "√âchec de g√©n√©ration");
      }
    } catch (error: unknown) {
      console.error("‚ùå Erreur g√©n√©ration:", error);
      const errorMessage = new AIMessage(
        `üòî Oups, j'ai eu un probl√®me lors de la g√©n√©ration...\n${error instanceof Error ? error.message : 'Erreur inconnue'}\n\n` +
        `Pas de panique ! Tape "oui" pour r√©essayer ou "nouveau" pour recommencer avec un autre projet.`
      );

      return {
        messages: [errorMessage],
        conversationState: { phase: 'error', step: 5 },
        historyLength: 5,
        audioGenerated: false
      };
    }
  },

  getVoiceDisplayName(voiceName: string): string {
    const voiceDisplayNames: Record<string, string> = {
      'aoede': 'Aoede (voix f√©minine chaleureuse)',
      'achernar': 'Achernar (voix masculine forte)',
      'callirrhoe': 'Callirrhoe (voix jeune et dynamique)',
      'charon': 'Charon (voix grave et myst√©rieuse)',
      'despina': 'Despina (voix moderne et claire)',
      'orus': 'Orus (voix masculine professionnelle)',
      'pulcherrima': 'Pulcherrima (voix √©l√©gante)',
      'vindemiatrix': 'Vindemiatrix (voix expressive)',
      'zephyr': 'Zephyr (voix apaisante)',
      'sadachbia': 'Sadachbia (voix traditionnelle)',
      'fenrir': 'Fenrir (voix dramatique)'
    };

    return voiceDisplayNames[voiceName] || voiceName;
  },

  getStyleDisplayName(style: string): string {
    const styleNames: Record<string, string> = {
      'professional': 'Professionnel',
      'warm': 'Chaleureux',
      'energetic': 'Dynamique',
      'calm': 'Calme',
      'dramatic': 'Dramatique',
      'neutral': 'Naturel'
    };

    return styleNames[style] || style;
  },

  clearHistory(threadId: string): void {
    sessionStore.delete(threadId);
    conversationHistory.delete(threadId);
    console.log("üóëÔ∏è Session supprim√©e:", threadId);
  }
};

console.log("‚úÖ Agent audio intelligent cr√©√©");