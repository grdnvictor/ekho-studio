// backend/src/agents/audio/audio-agent.ts

import { ChatOpenAI } from "@langchain/openai";
import { MemorySaver } from "@langchain/langgraph";
import { SystemMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { audioGenerationTool } from "./tools/audio-generation";
import { recommendVoice } from "../../services/audio/VoiceGuide";

console.log("üöÄ Initialisation de l'agent audio intelligent...");

// URL de LM Studio depuis la variable d'environnement
const LM_STUDIO_URL = process.env.LM_STUDIO_URL || 'http://localhost:1234/v1';
console.log("üåç URL LM Studio:", LM_STUDIO_URL);

const agentModel = new ChatOpenAI({
  temperature: 0.7,
  model: "local-model",
  apiKey: "lm-studio",
  configuration: {
    baseURL: LM_STUDIO_URL
  }
});

// Ajouter les outils √† l'agent
const agentWithTools = agentModel.bindTools([audioGenerationTool]);

// Interface pour les donn√©es collect√©es de mani√®re flexible
interface CollectedData {
  projectType?: string;
  textContent?: string;
  targetAudience?: string;
  voiceGender?: string;
  emotionStyle?: string;
  duration?: number;
  context?: string;
  isReadyToGenerate?: boolean;
  conversationContext?: string[];
}

// Interface pour la r√©ponse de l'agent
interface AgentResponse {
  messages: AIMessage[];
  conversationState: {
    phase: string;
    step: number;
  };
  historyLength: number;
  audioGenerated: boolean;
  audioUrl?: string;
  collectedInfo?: string[];
  sessionData?: CollectedData;
}

// Stockage en m√©moire des sessions avec contexte conversationnel
const sessionStore = new Map<string, CollectedData>();
const conversationHistory = new Map<string, { role: string; content: string }[]>();

// Prompts optimis√©s pour une conversation naturelle
const CONVERSATION_PROMPTS = {
  welcome: [
    "üéôÔ∏è Salut ! Je suis ton assistant audio d'Ekho Studio. Dis-moi, quel type de contenu audio tu veux cr√©er aujourd'hui ?",
    "üéµ Bienvenue sur Ekho Studio ! Je suis l√† pour t'aider √† cr√©er l'audio parfait. Qu'est-ce que tu as en t√™te ?",
    "üëã Hello ! Pr√™t(e) √† cr√©er quelque chose d'incroyable ? Raconte-moi ton projet audio !",
  ],

  clarification: {
    text: [
      "Super ! Et quel est le texte que tu veux transformer en audio ?",
      "G√©nial ! Maintenant, partage-moi le texte que tu veux vocaliser.",
      "Parfait ! Quel message veux-tu faire passer ?"
    ],
    audience: [
      "C'est not√© ! Pour qui est destin√© cet audio ?",
      "Excellent choix ! Qui va √©couter ton audio ?",
      "Top ! √Ä quel public s'adresse ton message ?"
    ],
    voice: [
      "Compris ! Quel type de voix pr√©f√®res-tu ?",
      "D'accord ! Tu pr√©f√®res une voix masculine, f√©minine, ou peu importe ?",
      "Ok ! As-tu une pr√©f√©rence pour le type de voix ?"
    ],
    style: [
      "Presque fini ! Quel style veux-tu donner √† ton audio ?",
      "Derni√®re touche : quelle ambiance souhaites-tu ?",
      "Et pour finir, quel ton veux-tu adopter ?"
    ]
  },

  encouragement: [
    "C'est un super projet !",
    "J'adore ton id√©e !",
    "√áa va √™tre g√©nial !",
    "Excellent choix !",
    "Tu as bon go√ªt !",
    "C'est exactement ce qu'il faut !",
    "Wow, j'ai h√¢te d'entendre le r√©sultat !"
  ],

  readyToGenerate: [
    "üöÄ Parfait ! J'ai tout ce qu'il me faut. On lance la cr√©ation de ton audio ?",
    "‚ú® Super ! Tout est pr√™t. Je peux g√©n√©rer ton audio maintenant ?",
    "üéØ Excellent ! J'ai toutes les infos. On y va ?"
  ]
};

// Fonction pour d√©tecter si un message contient du texte √† vocaliser
function detectTextContent(message: string): string | null {
  // Nettoyer le message
  const cleanedMessage = message.trim();
  
  // Patterns pour d√©tecter le texte
  const textPatterns = [
    /^["¬´](.+)["¬ª]$/,  // Entre guillemets
    /^[''](.+)['']$/,  // Entre apostrophes
    /le texte est\s*:?\s*(.+)/i,
    /voici le texte\s*:?\s*(.+)/i,
    /^(.+)$/  // Tout le message si aucun pattern
  ];

  for (const pattern of textPatterns) {
    const match = cleanedMessage.match(pattern);
    if (match && match[1]) {
      const extractedText = match[1].trim().replace(/^["¬´'']|["¬ª'']$/g, '');
      // V√©rifier que c'est bien du texte √† vocaliser (plus de 3 mots ou entre guillemets)
      if (extractedText.split(/\s+/).length > 3 || /["¬´'']/.test(cleanedMessage)) {
        return extractedText;
      }
    }
  }

  // Si le message fait plus de 20 caract√®res et ne ressemble pas √† une question
  if (cleanedMessage.length > 20 && !cleanedMessage.includes('?') && !cleanedMessage.toLowerCase().includes('je veux')) {
    return cleanedMessage;
  }

  return null;
}

// Analyser intelligemment le message utilisateur avec LLM
async function analyzeUserMessageWithLLM(
  message: string, 
  sessionData: CollectedData,
  conversationContext: { role: string; content: string }[]
): Promise<{
  detectedInfo: Partial<CollectedData>;
  confidence: number;
  intentRecognized: string;
}> {
  const prompt = `Tu es un assistant qui analyse les messages dans une conversation de cr√©ation audio.
Analyse ce message et l'historique pour extraire les informations pertinentes.

Historique de conversation:
${conversationContext.slice(-3).map(msg => `${msg.role}: ${msg.content}`).join('\n')}

Nouveau message: "${message}"

Donn√©es d√©j√† collect√©es: ${JSON.stringify(sessionData)}

IMPORTANT: Si le message contient du texte entre guillemets ou semble √™tre le texte √† vocaliser (plus de 20 caract√®res sans √™tre une question), extrais-le comme textContent.

Retourne UNIQUEMENT un JSON avec:
{
  "detectedInfo": {
    "projectType": "type si mentionn√© (publicit√©/podcast/formation/etc)",
    "textContent": "texte √† vocaliser si fourni",
    "targetAudience": "public cible si mentionn√©",
    "voiceGender": "masculine/feminine si pr√©f√©rence exprim√©e",
    "emotionStyle": "style √©motionnel si mentionn√©"
  },
  "confidence": 0.0 √† 1.0,
  "intentRecognized": "provide_text/provide_info/confirm/modify/other"
}`;

  try {
    const response = await agentModel.invoke(prompt);
    const content = response.content as string;
    const jsonMatch = content.match(/\{[\s\S]*\}/);
    
    if (jsonMatch) {
      return JSON.parse(jsonMatch[0]);
    }
  } catch (error) {
    console.error("Erreur analyse LLM:", error);
  }

  // Fallback avec d√©tection basique
  const detectedText = detectTextContent(message);
  return {
    detectedInfo: detectedText ? { textContent: detectedText } : {},
    confidence: detectedText ? 0.8 : 0.2,
    intentRecognized: detectedText ? 'provide_text' : 'other'
  };
}

// G√©n√©rer la prochaine r√©ponse avec le LLM
async function generateNextResponse(
  sessionData: CollectedData,
  conversationContext: { role: string; content: string }[],
  lastUserIntent: string
): Promise<string> {
  const missingInfo = getMissingInfo(sessionData);
  
  const prompt = `Tu es l'assistant audio d'Ekho Studio, chaleureux et professionnel.

Contexte de conversation:
${conversationContext.slice(-4).map(msg => `${msg.role}: ${msg.content}`).join('\n')}

Donn√©es collect√©es: ${JSON.stringify(sessionData)}
Informations manquantes: ${missingInfo.join(', ')}
Derni√®re intention utilisateur: ${lastUserIntent}

R√àGLES IMPORTANTES:
1. Ne JAMAIS redemander une information d√©j√† fournie
2. Si le texte est fourni, passer √† l'information suivante
3. Une seule question √† la fois
4. √ätre naturel et encourageant
5. Si toutes les infos essentielles sont l√†, proposer de g√©n√©rer
6. Ne JAMAIS utiliser de markdown (pas de **, __, *, etc.)
7. Si le texte fourni est court, proposer de l'am√©liorer

G√©n√®re la prochaine r√©ponse appropri√©e, en langage naturel sans formatage.`;

  try {
    const response = await agentModel.invoke(prompt);
    return response.content as string;
  } catch (error) {
    console.error("Erreur g√©n√©ration r√©ponse:", error);
    // Fallback
    if (!sessionData.textContent) {
      return getRandomResponse(CONVERSATION_PROMPTS.clarification.text);
    } else if (missingInfo.length > 0) {
      return `${getRandomResponse(CONVERSATION_PROMPTS.encouragement)} Pour am√©liorer le r√©sultat, ${missingInfo[0].toLowerCase()} ?`;
    } else {
      return getRandomResponse(CONVERSATION_PROMPTS.readyToGenerate);
    }
  }
}

// D√©terminer ce qui manque pour g√©n√©rer
function getMissingInfo(sessionData: CollectedData): string[] {
  const missing: string[] = [];

  if (!sessionData.textContent) {
    missing.push('Le texte √† vocaliser');
  }

  // Les autres infos sont optionnelles mais am√©liorent le r√©sultat
  if (!sessionData.targetAudience) {
    missing.push('Pour qui est destin√© cet audio');
  }
  if (!sessionData.voiceGender && !sessionData.emotionStyle) {
    missing.push('Quel style de voix tu pr√©f√®res');
  }

  return missing;
}

// Choisir une r√©ponse appropri√©e
function getRandomResponse(responses: string[]): string {
  return responses[Math.floor(Math.random() * responses.length)];
}

export const audioAgent = {
  async invoke(input: any, config: any): Promise<AgentResponse> {
    console.log("ü§ñ Agent intelligent appel√©");

    const threadId: string = config.configurable?.thread_id;
    if (!threadId) {
      throw new Error("Thread ID manquant");
    }

    // R√©cup√©rer ou initialiser les donn√©es de session
    let sessionData: CollectedData = sessionStore.get(threadId) || {};
    let history = conversationHistory.get(threadId) || [];

    // Extraire le message utilisateur
    const userMessage = input.messages?.[input.messages.length - 1];
    const userText: string = userMessage?.content || '';

    console.log("üìä √âtat session:", {
      hasText: !!sessionData.textContent,
      hasProjectType: !!sessionData.projectType,
      historyLength: history.length,
      userMessage: userText.slice(0, 50)
    });

    try {
      // Si c'est le premier message
      if (history.length === 0) {
        const welcomeMessage = new AIMessage(getRandomResponse(CONVERSATION_PROMPTS.welcome));
        history.push({ role: 'assistant', content: welcomeMessage.content as string });
        conversationHistory.set(threadId, history);

        return {
          messages: [welcomeMessage],
          conversationState: { phase: 'discovery', step: 1 },
          historyLength: 1,
          audioGenerated: false
        };
      }

      // Ajouter le message utilisateur √† l'historique
      history.push({ role: 'user', content: userText });

      // Analyser le message utilisateur avec LLM
      const analysis = await analyzeUserMessageWithLLM(userText, sessionData, history);
      const { detectedInfo, confidence, intentRecognized } = analysis;

      console.log("üîç Analyse du message:", {
        detectedInfo,
        confidence,
        intentRecognized
      });

      // Mettre √† jour les donn√©es de session avec les infos d√©tect√©es
      sessionData = { ...sessionData, ...detectedInfo };
      sessionStore.set(threadId, sessionData);

      // Si on a le texte principal et l'utilisateur confirme, on peut g√©n√©rer
      if (sessionData.textContent && 
          (intentRecognized === 'confirm' || 
           userText.toLowerCase().match(/oui|go|lance|g√©n√®re|ok|parfait/))) {
        console.log("üéµ G√©n√©ration audio demand√©e");
        
        // D'abord envoyer un message de statut
        const statusMessage = new AIMessage(
          "üéµ Super ! Je lance la g√©n√©ration de ton audio... √áa va prendre quelques secondes ‚è≥"
        );
        history.push({ role: 'assistant', content: statusMessage.content as string });
        conversationHistory.set(threadId, history);
        
        return await this.generateAudio(sessionData, threadId);
      }

      // G√©n√©rer la prochaine r√©ponse avec le LLM
      const response = await generateNextResponse(sessionData, history, intentRecognized);
      
      // D√©terminer la phase
      let phase = 'clarification';
      if (!sessionData.textContent) {
        phase = 'discovery';
      } else if (getMissingInfo(sessionData).length === 0) {
        phase = 'generation';
      }

      // Cr√©er le message de r√©ponse
      const responseMessage = new AIMessage(response);
      history.push({ role: 'assistant', content: response });
      conversationHistory.set(threadId, history);

      // Collecter les infos pour l'affichage
      const collectedInfo: string[] = [];
      if (sessionData.projectType) collectedInfo.push(`Type: ${sessionData.projectType}`);
      if (sessionData.textContent) collectedInfo.push('Texte fourni ‚úì');
      if (sessionData.targetAudience) collectedInfo.push(`Public: ${sessionData.targetAudience}`);
      if (sessionData.voiceGender) collectedInfo.push(`Voix: ${sessionData.voiceGender}`);
      if (sessionData.emotionStyle) collectedInfo.push(`Style: ${sessionData.emotionStyle}`);

      return {
        messages: [responseMessage],
        conversationState: { phase, step: history.length },
        historyLength: history.length,
        audioGenerated: false,
        collectedInfo,
        sessionData
      };

    } catch (error: unknown) {
      console.error("‚ùå Erreur dans l'agent:", error);
      const errorMessage = new AIMessage(
        "üòÖ Oups ! J'ai eu un petit souci. Peux-tu reformuler ta demande ?"
      );

      return {
        messages: [errorMessage],
        conversationState: { phase: 'error', step: history.length },
        historyLength: history.length,
        audioGenerated: false
      };
    }
  },

  async generateAudio(sessionData: CollectedData, threadId: string): Promise<AgentResponse> {
    console.log("üéµ G√©n√©ration audio avec donn√©es:", sessionData);

    try {
      if (!sessionData.textContent) {
        throw new Error("Contenu textuel manquant");
      }

      // Choisir la voix optimale intelligemment
      const voiceName: string = recommendVoice({
        projectType: sessionData.projectType,
        targetAudience: sessionData.targetAudience,
        emotion: sessionData.emotionStyle,
        gender: sessionData.voiceGender
      });

      // Pr√©parer les param√®tres
      const generationParams = {
        text: sessionData.textContent,
        voiceName: voiceName,
        emotion: sessionData.emotionStyle || 'neutral',
        speed: 1.0,
        effects: []
      };

      console.log("üéØ Param√®tres g√©n√©ration:", generationParams);

      // G√©n√©rer l'audio
      const audioResult = await audioGenerationTool.invoke(generationParams);

      if (audioResult.success) {
        const funMessages = [
          "üéâ Tadaaa ! Ton audio est pr√™t ! J'esp√®re qu'il te plaira autant qu'√† moi !",
          "‚ú® Et voil√† ! J'ai mis tout mon c≈ìur dans cet audio. √âcoute-le vite !",
          "üöÄ Mission accomplie ! Ton audio est fin pr√™t. C'est du lourd !",
          "üéµ Boom ! Audio g√©n√©r√© avec succ√®s ! J'ai h√¢te que tu l'√©coutes !"
        ];

        const summaryMessage = new AIMessage(
          `${getRandomResponse(funMessages)}\n\n` +
          `üìã Petit r√©cap de ton projet :\n` +
          (sessionData.projectType ? `‚Ä¢ Type : ${sessionData.projectType}\n` : '') +
          (sessionData.targetAudience ? `‚Ä¢ Public : ${sessionData.targetAudience}\n` : '') +
          `‚Ä¢ Voix : ${this.getVoiceDisplayName(voiceName)}\n` +
          (sessionData.emotionStyle ? `‚Ä¢ Style : ${this.getStyleDisplayName(sessionData.emotionStyle)}\n` : '') +
          `‚Ä¢ Dur√©e : ~${('duration' in audioResult ? audioResult.duration : 0)}s\n\n` +
          `üéß Tu peux maintenant √©couter et t√©l√©charger ton audio ci-dessus.\n\n` +
          `üí° Envie d'autre chose ? Dis-moi "nouveau" pour cr√©er un autre audio ou explique-moi ce que tu veux modifier !`
        );

        // Reset pour un nouveau projet
        sessionStore.delete(threadId);
        conversationHistory.delete(threadId);

        return {
          messages: [summaryMessage],
          conversationState: { phase: 'complete', step: 6 },
          historyLength: 6,
          audioGenerated: true,
          audioUrl: ('url' in audioResult) ? audioResult.url : undefined,
          sessionData: sessionData
        };
      } else {
        throw new Error(('error' in audioResult ? audioResult.error : audioResult.message) || "√âchec de g√©n√©ration");
      }
    } catch (error: unknown) {
      console.error("‚ùå Erreur g√©n√©ration:", error);
      const errorMessage = new AIMessage(
        `üòî Oups, j'ai eu un probl√®me lors de la g√©n√©ration...\n${error instanceof Error ? error.message : 'Erreur inconnue'}\n\n` +
        `Pas de panique ! Tape "oui" pour r√©essayer ou "nouveau" pour recommencer avec un autre projet.`
      );

      return {
        messages: [errorMessage],
        conversationState: { phase: 'error', step: 5 },
        historyLength: 5,
        audioGenerated: false
      };
    }
  },

  getVoiceDisplayName(voiceName: string): string {
    const voiceDisplayNames: Record<string, string> = {
      'aoede': 'Aoede (voix f√©minine chaleureuse)',
      'achernar': 'Achernar (voix masculine forte)',
      'callirrhoe': 'Callirrhoe (voix jeune et dynamique)',
      'charon': 'Charon (voix grave et myst√©rieuse)',
      'despina': 'Despina (voix moderne et claire)',
      'orus': 'Orus (voix masculine professionnelle)',
      'pulcherrima': 'Pulcherrima (voix √©l√©gante)',
      'vindemiatrix': 'Vindemiatrix (voix expressive)',
      'zephyr': 'Zephyr (voix apaisante)',
      'sadachbia': 'Sadachbia (voix traditionnelle)',
      'fenrir': 'Fenrir (voix dramatique)'
    };

    return voiceDisplayNames[voiceName] || voiceName;
  },

  getStyleDisplayName(style: string): string {
    const styleNames: Record<string, string> = {
      'professional': 'Professionnel',
      'warm': 'Chaleureux',
      'energetic': 'Dynamique',
      'calm': 'Calme',
      'dramatic': 'Dramatique',
      'neutral': 'Naturel'
    };

    return styleNames[style] || style;
  },

  // M√©thode pour vider l'historique d'une session
  clearHistory(threadId: string): void {
    sessionStore.delete(threadId);
    conversationHistory.delete(threadId);
    console.log("üóëÔ∏è Session supprim√©e:", threadId);
  }
};

console.log("‚úÖ Agent audio intelligent cr√©√©");