// backend/src/agents/audio/audio-agent.ts

import { ChatOpenAI } from "@langchain/openai";
import { MemorySaver } from "@langchain/langgraph";
import { SystemMessage, HumanMessage, AIMessage } from "@langchain/core/messages";
import { audioGenerationTool } from "./tools/audio-generation";

console.log("üöÄ Initialisation de l'agent audio intelligent...");

// URL de LM Studio depuis la variable d'environnement
const LM_STUDIO_URL = process.env.LM_STUDIO_URL || 'http://localhost:1234/v1';
console.log("üåç URL LM Studio:", LM_STUDIO_URL);

const agentModel = new ChatOpenAI({
  temperature: 0.7,
  model: "local-model",
  apiKey: "lm-studio",
  configuration: {
    baseURL: LM_STUDIO_URL
  }
});

// Ajouter les outils √† l'agent
const agentWithTools = agentModel.bindTools([audioGenerationTool]);

const agentCheckpointer = new MemorySaver();

// Interface pour le contexte de conversation am√©lior√©
interface ConversationContext {
  textContent?: string;
  voicePreference?: string;
  emotionStyle?: string;
  targetAudience?: string;
  projectType?: string;
  duration?: number;
  speed?: number;
  additionalRequirements?: string[];
}

// Stockage en m√©moire des conversations avec contexte
const conversationStore = new Map<string, {
  messages: any[];
  context: ConversationContext;
}>();

export const audioAgent = {
  async invoke(input: any, config: any) {
    console.log("ü§ñ Agent invoke appel√© avec:", {
      messagesCount: input.messages?.length,
      threadId: config.configurable?.thread_id
    });

    const threadId = config.configurable?.thread_id;
    if (!threadId) {
      throw new Error("Thread ID manquant");
    }

    // R√©cup√©rer ou initialiser l'historique et le contexte pour cette session
    let sessionData = conversationStore.get(threadId) || {
      messages: [],
      context: {}
    };

    // Ajouter les nouveaux messages √† l'historique
    if (input.messages && input.messages.length > 0) {
      const lastMessage = input.messages[input.messages.length - 1];
      if (lastMessage._getType() === "human") {
        sessionData.messages.push(lastMessage);

        // Extraire et mettre √† jour le contexte √† partir du message
        sessionData.context = this.updateContextFromMessage(
          lastMessage.content,
          sessionData.context,
          sessionData.messages
        );

        conversationStore.set(threadId, sessionData);
      }
    }

    console.log("üìö Session data:", {
      messagesCount: sessionData.messages.length,
      context: sessionData.context
    });

    // Analyser l'√©tat de la conversation avec le contexte enrichi
    const conversationState = this.analyzeConversationStateWithContext(
      sessionData.messages,
      sessionData.context
    );
    console.log("üîç √âtat conversation enrichi:", conversationState);

    // G√©n√©rer le prompt syst√®me bas√© sur l'√©tat et le contexte
    const systemPrompt = this.generateContextualPrompt(conversationState, sessionData.context);

    // Construire la conversation compl√®te
    const fullConversation = [
      new SystemMessage(systemPrompt),
      ...sessionData.messages
    ];

    console.log("üìû Envoi √† LM Studio avec", fullConversation.length, "messages...");

    try {
      // D√©terminer si on doit g√©n√©rer de l'audio avec contexte enrichi
      const shouldGenerateAudio = this.shouldTriggerGeneration(conversationState, sessionData.context);

      if (shouldGenerateAudio) {
        console.log("üéµ Conditions r√©unies pour g√©n√©ration audio avec contexte");

        try {
          // Pr√©parer les param√®tres de g√©n√©ration bas√©s sur le contexte
          const generationParams = this.prepareGenerationParams(sessionData.context, sessionData.messages);

          console.log("üéØ G√©n√©ration audio avec param√®tres:", generationParams);

          // Appeler l'outil de g√©n√©ration audio
          const audioResult = await audioGenerationTool.invoke(generationParams);

          console.log("‚úÖ R√©sultat g√©n√©ration:", audioResult);

          if (audioResult.success) {
            const audioResponse = new AIMessage(
              `üéâ Parfait ! J'ai g√©n√©r√© votre audio avec succ√®s !\n\n` +
              `üìã **R√©capitulatif** :\n` +
              `‚Ä¢ Texte : "${generationParams.text.slice(0, 100)}${generationParams.text.length > 100 ? '...' : ''}"\n` +
              `‚Ä¢ Voix : ${generationParams.voiceName}\n` +
              `‚Ä¢ Style : ${generationParams.emotion || 'naturel'}\n` +
              `‚Ä¢ Vitesse : ${generationParams.speed}x\n` +
              `‚Ä¢ Dur√©e : ~${audioResult.duration}s\n\n` +
              `üéß Vous pouvez maintenant √©couter votre audio ci-dessous : ${audioResult.url}\n\n` +
              `Si vous souhaitez des ajustements (vitesse, style, voix diff√©rente), faites-le moi savoir !`
            );

            sessionData.messages.push(audioResponse);
            conversationStore.set(threadId, sessionData);

            return {
              messages: [audioResponse],
              conversationState: { ...conversationState, phase: 'complete' },
              historyLength: sessionData.messages.length,
              audioGenerated: true,
              audioUrl: audioResult.url,
              context: sessionData.context
            };
          } else {
            throw new Error(audioResult.error || "√âchec de g√©n√©ration");
          }
        } catch (audioError) {
          console.error("‚ùå Erreur g√©n√©ration audio:", audioError);
          const errorResponse = new AIMessage(
            `‚ùå Je rencontre une difficult√© technique pour g√©n√©rer l'audio. L'erreur est : ${audioError.message}\n\n` +
            `Pouvons-nous r√©essayer ? Ou souhaitez-vous ajuster quelque chose ?`
          );

          sessionData.messages.push(errorResponse);
          conversationStore.set(threadId, sessionData);

          return {
            messages: [errorResponse],
            conversationState,
            historyLength: sessionData.messages.length,
            audioGenerated: false,
            context: sessionData.context
          };
        }
      }

      // R√©ponse normale avec l'agent LLM
      const response = await agentWithTools.invoke(fullConversation);
      console.log("‚úÖ R√©ponse re√ßue de LM Studio");

      // Ajouter la r√©ponse √† l'historique
      const aiResponse = new AIMessage(response.content as string);
      sessionData.messages.push(aiResponse);
      conversationStore.set(threadId, sessionData);

      return {
        messages: [response],
        conversationState,
        historyLength: sessionData.messages.length,
        audioGenerated: false,
        context: sessionData.context
      };
    } catch (error) {
      console.error("‚ùå Erreur LM Studio:", error);
      throw error;
    }
  },

  /**
   * Met √† jour le contexte bas√© sur le message utilisateur
   */
  updateContextFromMessage(messageContent: string, currentContext: ConversationContext, messageHistory: any[]): ConversationContext {
    const content = messageContent.toLowerCase();
    const newContext = { ...currentContext };

    // Extraire le contenu √† vocaliser
    if (this.containsContentToVocalize(messageContent)) {
      newContext.textContent = this.extractTextContent(messageContent, messageHistory);
    }

    // D√©tecter le type de projet
    if (content.includes('radio') || content.includes('publicit√©')) {
      newContext.projectType = 'radio';
      newContext.targetAudience = 'grand public';
    } else if (content.includes('formation') || content.includes('elearning') || content.includes('cours')) {
      newContext.projectType = 'elearning';
      newContext.targetAudience = 'apprenants';
    } else if (content.includes('podcast')) {
      newContext.projectType = 'podcast';
    } else if (content.includes('documentaire')) {
      newContext.projectType = 'documentaire';
      newContext.emotionStyle = 'narratif';
    }

    // D√©tecter les pr√©f√©rences de voix
    if (content.includes('masculin') || content.includes('homme')) {
      newContext.voicePreference = 'achernar'; // Voix masculine forte
    } else if (content.includes('f√©minin') || content.includes('femme')) {
      newContext.voicePreference = 'aoede'; // Voix f√©minine chaleureuse
    } else if (content.includes('jeune')) {
      newContext.voicePreference = 'callirrhoe'; // Voix jeune
    } else if (content.includes('mature') || content.includes('profonde')) {
      newContext.voicePreference = 'charon'; // Voix profonde
    }

    // D√©tecter le style √©motionnel
    if (content.includes('chaleureux')) {
      newContext.emotionStyle = 'warm';
    } else if (content.includes('professionnel')) {
      newContext.emotionStyle = 'professional';
    } else if (content.includes('dynamique')) {
      newContext.emotionStyle = 'energetic';
    } else if (content.includes('calme')) {
      newContext.emotionStyle = 'calm';
    }

    // D√©tecter la dur√©e
    const durationMatch = content.match(/(\d+)\s*(seconde|minute|s|min)/);
    if (durationMatch) {
      let duration = parseInt(durationMatch[1]);
      if (durationMatch[2].includes('min')) {
        duration *= 60;
      }
      newContext.duration = duration;
    }

    // D√©tecter la vitesse
    if (content.includes('rapide') || content.includes('vite')) {
      newContext.speed = 1.2;
    } else if (content.includes('lent') || content.includes('pos√©')) {
      newContext.speed = 0.8;
    }

    // D√©tecter l'audience cible
    if (content.includes('enfant') || content.includes('jeune')) {
      newContext.targetAudience = 'jeunes';
    } else if (content.includes('adulte') || content.includes('professionnel')) {
      newContext.targetAudience = 'adultes professionnels';
    }

    return newContext;
  },

  /**
   * V√©rifie si le message contient du contenu √† vocaliser
   */
  containsContentToVocalize(content: string): boolean {
    const indicators = [
      'texte:', 'script:', 'contenu:', 'dire:', 'lire:',
      'voici le texte', 'voici le contenu', 'voici le script'
    ];

    const lowerContent = content.toLowerCase();
    return indicators.some(indicator => lowerContent.includes(indicator)) ||
      (content.length > 50 && !content.includes('?'));
  },

  /**
   * Extrait le texte √† vocaliser
   */
  extractTextContent(messageContent: string, messageHistory: any[]): string {
    // Chercher des patterns sp√©cifiques
    const patterns = [
      /(?:texte|script|contenu|dire|lire)\s*:\s*(.+)/i,
      /voici\s+(?:le\s+)?(?:texte|contenu|script)\s*:?\s*(.+)/i
    ];

    for (const pattern of patterns) {
      const match = messageContent.match(pattern);
      if (match) {
        return match[1].trim();
      }
    }

    // Si pas de pattern, prendre le message entier s'il est assez long
    if (messageContent.length > 50 && !messageContent.includes('?')) {
      return messageContent.trim();
    }

    // Fallback: chercher dans l'historique le message le plus long
    const userMessages = messageHistory.filter(msg => msg._getType() === 'human');
    const longestMessage = userMessages.reduce((longest, current) =>
        current.content.length > longest.content.length ? current : longest,
      { content: '' }
    );

    return longestMessage.content.length > 20 ? longestMessage.content : messageContent;
  },

  /**
   * Analyse l'√©tat de la conversation avec le contexte enrichi
   */
  analyzeConversationStateWithContext(history: any[], context: ConversationContext) {
    const state = {
      messageCount: history.length,
      hasContent: !!context.textContent,
      hasAudience: !!context.targetAudience,
      hasVoice: !!context.voicePreference,
      hasStyle: !!context.emotionStyle,
      hasContext: !!context.projectType,
      phase: 'discovery' as 'discovery' | 'clarification' | 'generation' | 'complete',
      collectedInfo: [] as string[],
      context
    };

    // Construire la liste des infos collect√©es
    if (state.hasContent) state.collectedInfo.push('Contenu √† vocaliser');
    if (state.hasAudience) state.collectedInfo.push('Public cible');
    if (state.hasVoice) state.collectedInfo.push('Type de voix');
    if (state.hasStyle) state.collectedInfo.push('Style/√©motion');
    if (state.hasContext) state.collectedInfo.push('Type de projet');

    // D√©terminer la phase
    const completedCount = [state.hasContent, state.hasAudience, state.hasVoice, state.hasStyle, state.hasContext]
      .filter(Boolean).length;

    if (completedCount === 0) {
      state.phase = 'discovery';
    } else if (completedCount < 3 || !state.hasContent) {
      state.phase = 'clarification';
    } else if (completedCount >= 3 && state.hasContent) {
      state.phase = 'generation';
    } else {
      state.phase = 'complete';
    }

    return state;
  },

  /**
   * D√©termine si on doit d√©clencher la g√©n√©ration audio
   */
  shouldTriggerGeneration(state: any, context: ConversationContext): boolean {
    return state.phase === 'generation' &&
      state.hasContent &&
      state.collectedInfo.length >= 3 &&
      !!context.textContent;
  },

  /**
   * Pr√©pare les param√®tres de g√©n√©ration bas√©s sur le contexte
   */
  prepareGenerationParams(context: ConversationContext, messageHistory: any[]) {
    return {
      text: context.textContent || this.extractTextFromHistory(messageHistory),
      voiceName: context.voicePreference || "aoede", // Voix par d√©faut fiable
      emotion: context.emotionStyle || "neutral",
      speed: context.speed || 1.0,
      effects: []
    };
  },

  /**
   * Fallback pour extraire le texte depuis l'historique
   */
  extractTextFromHistory(history: any[]): string {
    const userMessages = history.filter(msg => msg._getType() === 'human');

    for (const message of userMessages.reverse()) {
      if (message.content.length > 50 && !message.content.toLowerCase().includes('?')) {
        return message.content.trim();
      }
    }

    const longestMessage = userMessages.reduce((longest, current) =>
        current.content.length > longest.content.length ? current : longest,
      { content: 'Bonjour, voici un test audio.' }
    );

    return longestMessage.content;
  },

  /**
   * G√©n√®re un prompt contextuel intelligent
   */
  generateContextualPrompt(state: any, context: ConversationContext): string {
    const basePrompt = `Tu es l'Assistant Audio professionnel d'Ekho Studio, sp√©cialis√© dans la cr√©ation de contenus audio de qualit√©.

CONTEXTE ACTUEL:
- Phase: ${state.phase}
- Messages √©chang√©s: ${state.messageCount}
- Informations collect√©es: ${state.collectedInfo.join(', ') || 'Aucune'}

CONTEXTE D√âTECT√â:
${context.textContent ? `- Contenu: "${context.textContent.slice(0, 100)}..."` : ''}
${context.projectType ? `- Type de projet: ${context.projectType}` : ''}
${context.targetAudience ? `- Public cible: ${context.targetAudience}` : ''}
${context.voicePreference ? `- Voix pr√©f√©r√©e: ${context.voicePreference}` : ''}
${context.emotionStyle ? `- Style √©motionnel: ${context.emotionStyle}` : ''}
${context.duration ? `- Dur√©e souhait√©e: ${context.duration}s` : ''}

MISSION: Aider l'utilisateur √† cr√©er du contenu audio professionnel √©tape par √©tape.

IMPORTANT: Tu as acc√®s √† un outil de g√©n√©ration audio. Quand tu as assez d'informations (phase g√©n√©ration), tu peux proposer de cr√©er l'audio imm√©diatement.`;

    switch (state.phase) {
      case 'discovery':
        return `${basePrompt}

PHASE D√âCOUVERTE - Premier contact
Tu d√©couvres les besoins de l'utilisateur pour la premi√®re fois.

APPROCHE:
- Sois accueillant et professionnel
- Pose UNE question ouverte pour comprendre le projet global
- Demande soit le contenu √† vocaliser, soit le type de projet
- Ne submerge pas avec trop de questions

EXEMPLES DE QUESTIONS:
- "Quel type de contenu audio souhaitez-vous cr√©er ?"
- "Avez-vous d√©j√† un texte √† vocaliser ou voulez-vous que je vous aide √† le cr√©er ?"
- "Parlez-moi de votre projet audio"

Reste conversationnel et √©vite les listes √† puces.`;

      case 'clarification':
        const missingElements = [];
        if (!state.hasContent) missingElements.push('- Le contenu exact √† vocaliser');
        if (!state.hasAudience) missingElements.push('- Le public cible (√¢ge, contexte)');
        if (!state.hasVoice) missingElements.push('- Le type de voix souhait√© (masculin, f√©minin, etc.)');
        if (!state.hasStyle) missingElements.push('- Le style/ton souhait√© (professionnel, chaleureux, etc.)');
        if (!state.hasContext) missingElements.push('- Le contexte d\'utilisation (radio, web, formation, etc.)');

        return `${basePrompt}

PHASE CLARIFICATION - Collecte d'informations
L'utilisateur a donn√© quelques informations, il faut clarifier les d√©tails manquants.

INFORMATIONS ENCORE N√âCESSAIRES:
${missingElements.join('\n')}

APPROCHE:
- Pose UNE SEULE question √† la fois
- Choisis l'information la plus importante manquante
- Reste naturel et conversationnel
- Explique pourquoi cette info est utile

NE redemande JAMAIS ce qui a d√©j√† √©t√© fourni !`;

      case 'generation':
        return `${basePrompt}

PHASE G√âN√âRATION - Pr√™t √† cr√©er
La plupart des informations sont collect√©es, il est temps de proposer la g√©n√©ration.

INFORMATIONS COLLECT√âES: ${state.collectedInfo.join(', ')}

APPROCHE:
- R√©sume ce qui a √©t√© collect√©
- Propose de proc√©der √† la g√©n√©ration IMM√âDIATEMENT
- Sois confiant et enthousiaste
- Annonce que l'audio va √™tre cr√©√©

IMPORTANT: Tu peux maintenant g√©n√©rer l'audio r√©ellement ! L'outil va cr√©er un fichier MP3/WAV que l'utilisateur pourra √©couter.

EXEMPLE: "Parfait ! J'ai toutes les informations n√©cessaires. Je vais cr√©er votre audio [style] maintenant. G√©n√©ration en cours..."`;

      case 'complete':
        return `${basePrompt}

PHASE COMPL√àTE - Audio g√©n√©r√© ou pr√™t
L'audio a √©t√© g√©n√©r√© ou toutes les informations sont disponibles.

APPROCHE:
- Pr√©sente le r√©sultat
- Propose des ajustements si n√©cessaire
- Offre des options suppl√©mentaires (variations, autres versions)
- Sois fier du travail accompli`;

      default:
        return basePrompt;
    }
  },

  // M√©thode pour vider l'historique d'une session
  clearHistory(threadId: string) {
    conversationStore.delete(threadId);
    console.log("üóëÔ∏è Historique supprim√© pour:", threadId);
  }
};

console.log("‚úÖ Agent audio intelligent cr√©√©");